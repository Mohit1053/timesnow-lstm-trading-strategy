{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TECHNICAL INDICATORS CONFIGURATION =====\n",
    "\n",
    "# Data Processing Settings\n",
    "CHUNK_SIZE = 500000  # Number of rows to process at once for large datasets\n",
    "MIN_DATA_POINTS = 30  # Minimum number of data points required per company\n",
    "\n",
    "# Moving Average Periods\n",
    "SMA_PERIODS = [10, 20, 50]\n",
    "EMA_PERIODS = [12, 26, 50]\n",
    "\n",
    "# MACD Settings\n",
    "MACD_FAST = 12\n",
    "MACD_SLOW = 26\n",
    "MACD_SIGNAL = 9\n",
    "\n",
    "# RSI Settings\n",
    "RSI_PERIOD = 14\n",
    "\n",
    "# Bollinger Bands Settings\n",
    "BB_PERIOD = 20\n",
    "BB_STD_DEV = 2\n",
    "\n",
    "# Stochastic Settings\n",
    "STOCH_K_PERIOD = 14\n",
    "STOCH_D_PERIOD = 3\n",
    "\n",
    "# ATR Period\n",
    "ATR_PERIOD = 14\n",
    "\n",
    "# ADX Period\n",
    "ADX_PERIOD = 14\n",
    "\n",
    "# ROC Period\n",
    "ROC_PERIOD = 12\n",
    "\n",
    "# CCI Period\n",
    "CCI_PERIOD = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FILE PATHS & COLUMN MAPPINGS =====\n",
    "\n",
    "# File Paths\n",
    "DATA_RAW_PATH = \"data/raw/\"\n",
    "DATA_PROCESSED_PATH = \"data/processed/\"\n",
    "OUTPUT_PATH = \"output/\"\n",
    "\n",
    "# Column Mappings (for automatic detection)\n",
    "PRICE_DATE_COLS = ['date', 'time', 'pricedate']\n",
    "OPEN_COLS = ['open', 'openprice']\n",
    "HIGH_COLS = ['high', 'adjustedhighprice', 'highprice']\n",
    "LOW_COLS = ['low', 'adjustedlowprice', 'lowprice']\n",
    "CLOSE_COLS = ['close', 'adjustedcloseprice', 'closeprice']\n",
    "VOLUME_COLS = ['volume', 'quantity', 'traded', 'tradedquantity']\n",
    "\n",
    "# Data Quality Thresholds\n",
    "MAX_MISSING_PERCENTAGE = 50  # Skip companies if >50% of OHLC data is missing\n",
    "MIN_ROWS_PER_COMPANY = 50   # Minimum rows required per company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f20670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LSTM TRADING STRATEGY CONFIGURATION =====\n",
    "\n",
    "# Model Parameters\n",
    "SEQUENCE_LENGTH = 60          # Number of days to look back for LSTM input\n",
    "LSTM_UNITS = [100, 50]       # LSTM layer sizes [first_layer, second_layer, ...]\n",
    "DROPOUT_RATE = 0.2           # Dropout rate for regularization\n",
    "EPOCHS = 50                  # Training epochs\n",
    "BATCH_SIZE = 32              # Training batch size\n",
    "VALIDATION_SPLIT = 0.1       # Fraction of training data for validation\n",
    "\n",
    "# Trading Parameters\n",
    "TARGET_HORIZON = 5           # Days to hold position\n",
    "TARGET_GAIN = 0.10          # Target profit (10% = 0.10)\n",
    "STOP_LOSS = -0.03           # Stop loss (-3% = -0.03)\n",
    "TEST_SPLIT = 0.2            # Fraction of data for testing\n",
    "\n",
    "# Signal Generation\n",
    "TREND_THRESHOLD = 0.001     # Minimum price change to consider as trend (0.1%)\n",
    "CONFIDENCE_THRESHOLD = 0.6   # Minimum confidence for signal generation\n",
    "\n",
    "# Risk Management\n",
    "MAX_POSITION_SIZE = 1.0     # Maximum position size (1.0 = 100% of capital)\n",
    "RISK_FREE_RATE = 0.02       # Risk-free rate for Sharpe ratio calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92856cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PRIORITY INDICATORS CONFIGURATION =====\n",
    "\n",
    "# Primary feature (required)\n",
    "PRIMARY_FEATURE = 'close'\n",
    "\n",
    "# Technical indicators in priority order for LSTM training\n",
    "TECHNICAL_INDICATORS = [\n",
    "    # Momentum Indicators (Priority)\n",
    "    'RSI',              # Relative Strength Index\n",
    "    'ROC',              # Rate of Change  \n",
    "    'Stoch_K',          # Stochastic %K\n",
    "    'Stoch_D',          # Stochastic %D\n",
    "    'TSI',              # True Strength Index\n",
    "    \n",
    "    # Volume Indicators (Priority)\n",
    "    'OBV',              # On Balance Volume\n",
    "    'MFI',              # Money Flow Index\n",
    "    'PVT',              # Price Volume Trend\n",
    "    \n",
    "    # Trend Indicators (Priority) \n",
    "    'MACD',             # MACD Line\n",
    "    'TEMA',             # Triple Exponential Moving Average\n",
    "    'KAMA',             # Kaufman's Adaptive Moving Average\n",
    "    \n",
    "    # Volatility Indicators (Priority)\n",
    "    'ATR',              # Average True Range\n",
    "    'BB_Position',      # Bollinger Band Position\n",
    "    'Ulcer_Index',      # Ulcer Index\n",
    "    \n",
    "    # Additional Supporting Indicators\n",
    "    'ADX',              # Average Directional Index\n",
    "    'Volume_Ratio',     # Volume Ratio\n",
    "    'Price_Change'      # Price Change\n",
    "]\n",
    "\n",
    "# Performance Metrics to Track\n",
    "METRICS_TO_TRACK = [\n",
    "    'accuracy', 'precision', 'recall', 'f1_score', \n",
    "    'sharpe_ratio', 'max_drawdown', 'profit_factor', 'win_rate'\n",
    "]\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_MODEL = True\n",
    "SAVE_PLOTS = True\n",
    "PLOT_DPI = 300\n",
    "VERBOSE_TRAINING = 1\n",
    "\n",
    "print(f\"âœ… Priority indicators configured: {len(TECHNICAL_INDICATORS)} indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f220dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LSTM TRADING STRATEGY CONFIGURATION =====\n",
    "\n",
    "# LSTM Trading Strategy Configuration\n",
    "# Customize these parameters for your trading strategy\n",
    "\n",
    "# ===== MODEL PARAMETERS =====\n",
    "SEQUENCE_LENGTH = 60          # Number of days to look back for LSTM input\n",
    "LSTM_UNITS = [100, 50]       # LSTM layer sizes [first_layer, second_layer, ...]\n",
    "DROPOUT_RATE = 0.2           # Dropout rate for regularization\n",
    "EPOCHS = 50                  # Training epochs\n",
    "BATCH_SIZE = 32              # Training batch size\n",
    "VALIDATION_SPLIT = 0.1       # Fraction of training data for validation\n",
    "\n",
    "# ===== TRADING PARAMETERS =====\n",
    "TARGET_HORIZON = 5           # Days to hold position\n",
    "TARGET_GAIN = 0.10          # Target profit (10% = 0.10)\n",
    "STOP_LOSS = -0.03           # Stop loss (-3% = -0.03)\n",
    "TEST_SPLIT = 0.2            # Fraction of data for testing\n",
    "\n",
    "# ===== FEATURES TO USE =====\n",
    "# Primary feature (required)\n",
    "PRIMARY_FEATURE = 'close'\n",
    "\n",
    "# Additional technical indicators to include (if available)\n",
    "LSTM_TECHNICAL_INDICATORS = [\n",
    "    # Momentum Indicators (Priority)\n",
    "    'RSI',              # Relative Strength Index\n",
    "    'ROC',              # Rate of Change  \n",
    "    'Stoch_K',          # Stochastic %K\n",
    "    'Stoch_D',          # Stochastic %D\n",
    "    'TSI',              # True Strength Index\n",
    "    \n",
    "    # Volume Indicators (Priority)\n",
    "    'OBV',              # On Balance Volume\n",
    "    'MFI',              # Money Flow Index\n",
    "    'PVT',              # Price Volume Trend\n",
    "    \n",
    "    # Trend Indicators (Priority) \n",
    "    'MACD',             # MACD Line\n",
    "    'TEMA',             # Triple Exponential Moving Average\n",
    "    'KAMA',             # Kaufman's Adaptive Moving Average\n",
    "    \n",
    "    # Volatility Indicators (Priority)\n",
    "    'ATR',              # Average True Range\n",
    "    'BB_Position',      # Bollinger Band Position\n",
    "    'Ulcer_Index',      # Ulcer Index\n",
    "    \n",
    "    # Additional Supporting Indicators\n",
    "    'ADX',              # Average Directional Index\n",
    "    'Volume_Ratio',     # Volume Ratio\n",
    "    'Price_Change'      # Price Change\n",
    "]\n",
    "\n",
    "# ===== SIGNAL GENERATION =====\n",
    "TREND_THRESHOLD = 0.001     # Minimum price change to consider as trend (0.1%)\n",
    "CONFIDENCE_THRESHOLD = 0.6   # Minimum confidence for signal generation\n",
    "\n",
    "# ===== RISK MANAGEMENT =====\n",
    "MAX_POSITION_SIZE = 1.0     # Maximum position size (1.0 = 100% of capital)\n",
    "RISK_FREE_RATE = 0.02       # Risk-free rate for Sharpe ratio calculation\n",
    "\n",
    "# ===== OUTPUT SETTINGS =====\n",
    "SAVE_MODEL = True           # Whether to save the trained model\n",
    "SAVE_PLOTS = True           # Whether to save performance plots\n",
    "PLOT_DPI = 300              # Plot resolution\n",
    "VERBOSE_TRAINING = 1        # Training verbosity (0=silent, 1=progress bar, 2=epoch)\n",
    "\n",
    "# ===== DATA REQUIREMENTS =====\n",
    "MIN_DATA_POINTS = 1000      # Minimum data points required for training\n",
    "MIN_TEST_SAMPLES = 100      # Minimum samples in test set\n",
    "\n",
    "# ===== PERFORMANCE METRICS =====\n",
    "METRICS_TO_TRACK = [\n",
    "    'accuracy',             # Prediction accuracy\n",
    "    'precision',            # Signal precision\n",
    "    'recall',               # Signal recall\n",
    "    'f1_score',            # F1 score\n",
    "    'sharpe_ratio',        # Risk-adjusted returns\n",
    "    'max_drawdown',        # Maximum drawdown\n",
    "    'profit_factor',       # Profit factor\n",
    "    'win_rate'             # Win rate percentage\n",
    "]\n",
    "\n",
    "# ===== ADVANCED SETTINGS =====\n",
    "USE_EARLY_STOPPING = True   # Use early stopping during training\n",
    "EARLY_STOPPING_PATIENCE = 10  # Epochs to wait before stopping\n",
    "REDUCE_LR_PATIENCE = 5      # Epochs to wait before reducing learning rate\n",
    "LEARNING_RATE_FACTOR = 0.5  # Factor to reduce learning rate\n",
    "\n",
    "# ===== COMPANY FILTERING =====\n",
    "# Set to None to use first company, or specify company ID\n",
    "TARGET_COMPANY_ID = None\n",
    "\n",
    "# Alternative: Use top N companies by data volume\n",
    "USE_TOP_N_COMPANIES = None  # Set to integer to use top N companies\n",
    "\n",
    "# ===== FILE PATHS =====\n",
    "INPUT_DATA_PATH = \"data/processed/stock_data_with_technical_indicators.csv\"\n",
    "OUTPUT_PATH = \"output/\"\n",
    "MODEL_SAVE_PATH = \"output/models/\"\n",
    "\n",
    "# Output file names\n",
    "SIGNALS_OUTPUT_FILE = \"lstm_trade_signals.csv\"\n",
    "SUMMARY_OUTPUT_FILE = \"lstm_strategy_summary.csv\"\n",
    "REPORT_OUTPUT_FILE = \"lstm_strategy_report.txt\"\n",
    "PLOTS_OUTPUT_FILE = \"lstm_strategy_analysis.png\"\n",
    "MODEL_OUTPUT_FILE = \"lstm_trading_model.h5\"\n",
    "\n",
    "print(\"âœ… LSTM Trading Strategy configuration loaded\")\n",
    "print(f\"ðŸ“Š LSTM will use {len(LSTM_TECHNICAL_INDICATORS)} technical indicators\")\n",
    "print(f\"ðŸŽ¯ Training parameters: {EPOCHS} epochs, batch size {BATCH_SIZE}\")\n",
    "print(f\"ðŸ“ˆ Trading parameters: {TARGET_HORIZON} day horizon, {TARGET_GAIN*100}% target gain\")\n",
    "print(f\"ðŸ›¡ï¸  Risk management: {STOP_LOSS*100}% stop loss, max position {MAX_POSITION_SIZE*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imported required libraries for technical indicators calculation\")\n",
    "\n",
    "# ===== BASIC TECHNICAL INDICATORS =====\n",
    "\n",
    "def calculate_sma(data, window):\n",
    "    \"\"\"Simple Moving Average\"\"\"\n",
    "    return data.rolling(window=window).mean()\n",
    "\n",
    "def calculate_ema(data, window):\n",
    "    \"\"\"Exponential Moving Average\"\"\"\n",
    "    return data.ewm(span=window).mean()\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    \"\"\"Relative Strength Index with Wilder's smoothing\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    # Use Wilder's smoothing (exponential smoothing with alpha = 1/window)\n",
    "    alpha = 1.0 / window\n",
    "    avg_gain = gain.ewm(alpha=alpha, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_roc(data, window=12):\n",
    "    \"\"\"Rate of Change\"\"\"\n",
    "    return ((data - data.shift(window)) / data.shift(window)) * 100\n",
    "\n",
    "print(\"âœ… Basic technical indicators functions defined (SMA, EMA, RSI, ROC)\")\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Moving Average Convergence Divergence\"\"\"\n",
    "    ema_fast = calculate_ema(data, fast)\n",
    "    ema_slow = calculate_ema(data, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = calculate_ema(macd_line, signal)\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20, std_dev=2):\n",
    "    \"\"\"Bollinger Bands\"\"\"\n",
    "    sma = calculate_sma(data, window)\n",
    "    std = data.rolling(window=window).std()\n",
    "    upper_band = sma + (std * std_dev)\n",
    "    lower_band = sma - (std * std_dev)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_window=14, d_window=3):\n",
    "    \"\"\"Stochastic Oscillator\"\"\"\n",
    "    lowest_low = low.rolling(window=k_window).min()\n",
    "    highest_high = high.rolling(window=k_window).max()\n",
    "    k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))\n",
    "    d_percent = k_percent.rolling(window=d_window).mean()\n",
    "    return k_percent, d_percent\n",
    "\n",
    "def calculate_atr(high, low, close, window=14):\n",
    "    \"\"\"Average True Range\"\"\"\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    return atr\n",
    "\n",
    "def calculate_adx(high, low, close, window=14):\n",
    "    \"\"\"Average Directional Index with Wilder's smoothing\"\"\"\n",
    "    # Calculate True Range\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Calculate Directional Movement using pandas operations to maintain index alignment\n",
    "    high_diff = high - high.shift()\n",
    "    low_diff = low.shift() - low\n",
    "    \n",
    "    dm_plus = pd.Series(np.where(high_diff > low_diff, np.maximum(high_diff, 0), 0), index=high.index)\n",
    "    dm_minus = pd.Series(np.where(low_diff > high_diff, np.maximum(low_diff, 0), 0), index=high.index)\n",
    "    \n",
    "    # Use Wilder's smoothing (exponential smoothing with alpha = 1/window)\n",
    "    alpha = 1.0 / window\n",
    "    tr_smooth = tr.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_plus_smooth = dm_plus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_minus_smooth = dm_minus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    # Calculate DI+ and DI- with safe division\n",
    "    di_plus = 100 * (dm_plus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    di_minus = 100 * (dm_minus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    \n",
    "    # Calculate DX and ADX with safe division\n",
    "    di_sum = di_plus + di_minus\n",
    "    dx = 100 * abs(di_plus - di_minus) / di_sum.replace(0, np.nan)\n",
    "    adx = dx.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    return adx, di_plus, di_minus\n",
    "\n",
    "def calculate_parabolic_sar(high, low, af_start=0.02, af_max=0.2):\n",
    "    \"\"\"Parabolic SAR\"\"\"\n",
    "    length = len(high)\n",
    "    psar = np.zeros(length)\n",
    "    af = af_start\n",
    "    trend = 1  # 1 for uptrend, -1 for downtrend\n",
    "    ep = high.iloc[0]  # extreme point\n",
    "    \n",
    "    psar[0] = low.iloc[0]\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        if trend == 1:  # Uptrend\n",
    "            psar[i] = psar[i-1] + af * (ep - psar[i-1])\n",
    "            if low.iloc[i] < psar[i]:\n",
    "                trend = -1\n",
    "                psar[i] = ep\n",
    "                ep = low.iloc[i]\n",
    "                af = af_start\n",
    "            else:\n",
    "                if high.iloc[i] > ep:\n",
    "                    ep = high.iloc[i]\n",
    "                    af = min(af + af_start, af_max)\n",
    "        else:  # Downtrend\n",
    "            psar[i] = psar[i-1] - af * (psar[i-1] - ep)\n",
    "            if high.iloc[i] > psar[i]:\n",
    "                trend = 1\n",
    "                psar[i] = ep\n",
    "                ep = high.iloc[i]\n",
    "                af = af_start\n",
    "            else:\n",
    "                if low.iloc[i] < ep:\n",
    "                    ep = low.iloc[i]\n",
    "                    af = min(af + af_start, af_max)\n",
    "    \n",
    "    return pd.Series(psar, index=high.index)\n",
    "\n",
    "def calculate_ichimoku(high, low, close, tenkan=9, kijun=26, senkou_b=52):\n",
    "    \"\"\"Ichimoku Cloud\"\"\"\n",
    "    # Tenkan-sen (Conversion Line)\n",
    "    tenkan_sen = (high.rolling(window=tenkan).max() + low.rolling(window=tenkan).min()) / 2\n",
    "    \n",
    "    # Kijun-sen (Base Line)\n",
    "    kijun_sen = (high.rolling(window=kijun).max() + low.rolling(window=kijun).min()) / 2\n",
    "    \n",
    "    # Senkou Span A (Leading Span A)\n",
    "    senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(kijun)\n",
    "    \n",
    "    # Senkou Span B (Leading Span B)\n",
    "    senkou_span_b = ((high.rolling(window=senkou_b).max() + \n",
    "                      low.rolling(window=senkou_b).min()) / 2).shift(kijun)\n",
    "    \n",
    "    # Chikou Span (Lagging Span)\n",
    "    chikou_span = close.shift(-kijun)\n",
    "    \n",
    "    return tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b, chikou_span\n",
    "\n",
    "def calculate_bop(open_price, high, low, close):\n",
    "    \"\"\"Balance of Power\"\"\"\n",
    "    return (close - open_price) / (high - low)\n",
    "\n",
    "def calculate_cmf(high, low, close, volume, window=20):\n",
    "    \"\"\"Chaikin Money Flow (CMF)\"\"\"\n",
    "    if volume.isna().all() or close.isna().all():\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean data\n",
    "    clean_high = high.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_low = low.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_close = close.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_volume = volume.fillna(0)\n",
    "    \n",
    "    # Calculate Money Flow Multiplier\n",
    "    mf_multiplier = ((clean_close - clean_low) - (clean_high - clean_close)) / (clean_high - clean_low).replace(0, np.nan)\n",
    "    mf_multiplier = mf_multiplier.fillna(0)  # Handle division by zero\n",
    "    \n",
    "    # Calculate Money Flow Volume\n",
    "    mf_volume = mf_multiplier * clean_volume\n",
    "    \n",
    "    # Calculate CMF as moving average of Money Flow Volume\n",
    "    cmf = mf_volume.rolling(window=window).mean()\n",
    "    \n",
    "    return cmf\n",
    "\n",
    "def calculate_ema_indicator(close, window=9):\n",
    "    \"\"\"EMA Indicator (for trend detection)\"\"\"\n",
    "    ema = calculate_ema(close, window)\n",
    "    return (close - ema) / ema\n",
    "\n",
    "def calculate_mom(close, window=10):\n",
    "    \"\"\"Momentum\"\"\"\n",
    "    return close.diff(window)\n",
    "\n",
    "def calculate_obv(close, volume):\n",
    "    \"\"\"On-Balance Volume with missing data handling\"\"\"\n",
    "    if volume.isna().all() or close.isna().all():\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean data - forward fill missing values\n",
    "    clean_close = close.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_volume = volume.fillna(0)  # Fill missing volume with 0\n",
    "    \n",
    "    obv = np.zeros(len(clean_close))\n",
    "    if len(clean_volume) > 0:\n",
    "        obv[0] = clean_volume.iloc[0]\n",
    "    \n",
    "    for i in range(1, len(clean_close)):\n",
    "        if pd.isna(clean_close.iloc[i]) or pd.isna(clean_close.iloc[i-1]):\n",
    "            obv[i] = obv[i-1]  # No change if price data is missing\n",
    "        elif clean_close.iloc[i] > clean_close.iloc[i-1]:\n",
    "            obv[i] = obv[i-1] + clean_volume.iloc[i]\n",
    "        elif clean_close.iloc[i] < clean_close.iloc[i-1]:\n",
    "            obv[i] = obv[i-1] - clean_volume.iloc[i]\n",
    "        else:\n",
    "            obv[i] = obv[i-1]\n",
    "    \n",
    "    return pd.Series(obv, index=close.index)\n",
    "\n",
    "def calculate_ad_line(high, low, close, volume):\n",
    "    \"\"\"Accumulation/Distribution Line with missing data handling\"\"\"\n",
    "    if volume.isna().all() or high.isna().all() or low.isna().all() or close.isna().all():\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean data\n",
    "    clean_high = high.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_low = low.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_close = close.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_volume = volume.fillna(0)\n",
    "    \n",
    "    # Calculate Money Flow Multiplier with safe division\n",
    "    high_low_diff = clean_high - clean_low\n",
    "    mfm = ((clean_close - clean_low) - (clean_high - clean_close)) / high_low_diff.replace(0, np.nan)\n",
    "    mfm = mfm.fillna(0)  # Handle division by zero\n",
    "    \n",
    "    # Calculate Money Flow Volume\n",
    "    mfv = mfm * clean_volume\n",
    "    \n",
    "    # Calculate A/D Line as cumulative sum\n",
    "    ad_line = mfv.cumsum()\n",
    "    return ad_line\n",
    "\n",
    "def calculate_pivot_points(high, low, close):\n",
    "    \"\"\"Pivot Points\"\"\"\n",
    "    pivot = (high + low + close) / 3\n",
    "    r1 = 2 * pivot - low\n",
    "    s1 = 2 * pivot - high\n",
    "    r2 = pivot + (high - low)\n",
    "    s2 = pivot - (high - low)\n",
    "    r3 = high + 2 * (pivot - low)\n",
    "    s3 = low - 2 * (high - pivot)\n",
    "    \n",
    "    return pivot, r1, r2, r3, s1, s2, s3\n",
    "\n",
    "def add_fibonacci_levels(high, low, lookback=50):\n",
    "    \"\"\"Fibonacci Retracement Levels\"\"\"\n",
    "    rolling_high = high.rolling(window=lookback).max()\n",
    "    rolling_low = low.rolling(window=lookback).min()\n",
    "    diff = rolling_high - rolling_low\n",
    "    \n",
    "    fib_236 = rolling_high - 0.236 * diff\n",
    "    fib_382 = rolling_high - 0.382 * diff\n",
    "    fib_500 = rolling_high - 0.500 * diff\n",
    "    fib_618 = rolling_high - 0.618 * diff\n",
    "    fib_786 = rolling_high - 0.786 * diff\n",
    "    \n",
    "    return fib_236, fib_382, fib_500, fib_618, fib_786\n",
    "\n",
    "def calculate_volume_profile(close, volume, bins=20):\n",
    "    \"\"\"Volume Profile - Distribution of volume at different price levels\"\"\"\n",
    "    if len(close) < bins or volume.isna().all() or close.isna().all():\n",
    "        return pd.Series(index=close.index, dtype=float), pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean the data\n",
    "    valid_mask = ~(close.isna() | volume.isna() | (volume <= 0))\n",
    "    if valid_mask.sum() < bins:\n",
    "        return pd.Series(index=close.index, dtype=float), pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Initialize output series\n",
    "    volume_at_price = pd.Series(index=close.index, dtype=float)\n",
    "    dominant_price_level = pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    for i in range(len(close)):\n",
    "        if i < bins:\n",
    "            # Not enough data for meaningful volume profile\n",
    "            volume_at_price.iloc[i] = np.nan\n",
    "            dominant_price_level.iloc[i] = np.nan\n",
    "        else:\n",
    "            # Look at recent period for volume profile\n",
    "            start_idx = max(0, i - bins)\n",
    "            recent_close = close.iloc[start_idx:i+1]\n",
    "            recent_volume = volume.iloc[start_idx:i+1]\n",
    "            \n",
    "            # Filter out invalid data\n",
    "            valid_recent = ~(recent_close.isna() | recent_volume.isna() | (recent_volume <= 0))\n",
    "            \n",
    "            if valid_recent.sum() < 5:  # Need at least 5 valid points\n",
    "                volume_at_price.iloc[i] = np.nan\n",
    "                dominant_price_level.iloc[i] = np.nan\n",
    "                continue\n",
    "            \n",
    "            valid_close = recent_close[valid_recent]\n",
    "            valid_volume = recent_volume[valid_recent]\n",
    "            \n",
    "            # Create bins for this period\n",
    "            period_min = valid_close.min()\n",
    "            period_max = valid_close.max()\n",
    "            \n",
    "            if period_max > period_min:\n",
    "                try:\n",
    "                    period_bins = np.linspace(period_min, period_max, bins + 1)\n",
    "                    period_bin_indices = np.digitize(valid_close, period_bins) - 1\n",
    "                    period_bin_indices = np.clip(period_bin_indices, 0, bins - 1)\n",
    "                    \n",
    "                    # Sum volume for each bin\n",
    "                    bin_volumes = np.zeros(bins)\n",
    "                    for j, bin_idx in enumerate(period_bin_indices):\n",
    "                        bin_volumes[bin_idx] += valid_volume.iloc[j]\n",
    "                    \n",
    "                    # Find price level with highest volume\n",
    "                    max_volume_bin = np.argmax(bin_volumes)\n",
    "                    dominant_price = (period_bins[max_volume_bin] + period_bins[max_volume_bin + 1]) / 2\n",
    "                    \n",
    "                    volume_at_price.iloc[i] = bin_volumes[max_volume_bin]\n",
    "                    dominant_price_level.iloc[i] = dominant_price\n",
    "                except Exception:\n",
    "                    volume_at_price.iloc[i] = np.nan\n",
    "                    dominant_price_level.iloc[i] = np.nan\n",
    "            else:\n",
    "                volume_at_price.iloc[i] = valid_volume.sum()\n",
    "                dominant_price_level.iloc[i] = valid_close.iloc[-1]\n",
    "    \n",
    "    return volume_at_price, dominant_price_level\n",
    "\n",
    "def create_technical_indicators_dataset(file_path, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Main function to create a comprehensive technical indicators dataset\n",
    "    For very large datasets, set chunk_size to process data in smaller batches\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Try to read the CSV file with common column names\n",
    "    try:\n",
    "        if chunk_size:\n",
    "            print(f\"Processing data in chunks of {chunk_size} rows...\")\n",
    "            # Read first chunk to get column info\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "            first_chunk = next(chunk_iter)\n",
    "            print(f\"First chunk loaded. Shape: {first_chunk.shape}\")\n",
    "            print(f\"Columns: {first_chunk.columns.tolist()}\")\n",
    "            \n",
    "            # Process all chunks\n",
    "            all_chunks = [first_chunk]\n",
    "            for i, chunk in enumerate(chunk_iter, 2):\n",
    "                print(f\"Loading chunk {i}...\")\n",
    "                all_chunks.append(chunk)\n",
    "            \n",
    "            df = pd.concat(all_chunks, ignore_index=True)\n",
    "            print(f\"All chunks concatenated. Final shape: {df.shape}\")\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Auto-detect column names (case insensitive)\n",
    "    columns_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'date' in col_lower or 'time' in col_lower:\n",
    "            columns_map['date'] = col\n",
    "        elif 'open' in col_lower:\n",
    "            columns_map['open'] = col\n",
    "        elif 'high' in col_lower:\n",
    "            columns_map['high'] = col\n",
    "        elif 'low' in col_lower:\n",
    "            columns_map['low'] = col\n",
    "        elif 'close' in col_lower:\n",
    "            columns_map['close'] = col\n",
    "        elif 'volume' in col_lower or 'quantity' in col_lower or 'traded' in col_lower:\n",
    "            columns_map['volume'] = col\n",
    "    \n",
    "    print(f\"Detected columns mapping: {columns_map}\")\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    required_cols = ['open', 'high', 'low', 'close']\n",
    "    missing_cols = [col for col in required_cols if col not in columns_map]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing required columns: {missing_cols}\")\n",
    "        return None\n",
    "    \n",
    "    # Rename columns for easier access\n",
    "    df_renamed = df.rename(columns={v: k for k, v in columns_map.items()})\n",
    "    \n",
    "    # Convert date column if it exists\n",
    "    if 'date' in columns_map:\n",
    "        df_renamed['date'] = pd.to_datetime(df_renamed['date'])\n",
    "        df_renamed = df_renamed.sort_values('date')\n",
    "        df_renamed = df_renamed.set_index('date')\n",
    "    \n",
    "    # Ensure numeric columns\n",
    "    numeric_cols = ['open', 'high', 'low', 'close']\n",
    "    if 'volume' in columns_map:\n",
    "        numeric_cols.append('volume')\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df_renamed[col] = pd.to_numeric(df_renamed[col], errors='coerce')\n",
    "    \n",
    "    print(\"Calculating technical indicators...\")\n",
    "    \n",
    "    # For multi-company datasets, process by company to reduce memory usage\n",
    "    if 'companyid' in df_renamed.columns and len(df_renamed['companyid'].unique()) > 1:\n",
    "        result_df = process_by_company(df_renamed)\n",
    "    else:\n",
    "        # Process as single dataset\n",
    "        result_df = calculate_indicators_for_company(df_renamed)\n",
    "    \n",
    "    if result_df is None:\n",
    "        print(\"Failed to calculate technical indicators\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Technical indicators calculation completed!\")\n",
    "    print(f\"Final dataset shape: {result_df.shape}\")\n",
    "    print(f\"Number of indicators added: {result_df.shape[1] - df_renamed.shape[1]}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def process_by_company(df):\n",
    "    \"\"\"\n",
    "    Process technical indicators for each company separately to reduce memory usage\n",
    "    \"\"\"\n",
    "    print(\"Processing data by company groups...\")\n",
    "    \n",
    "    # Get unique companies\n",
    "    companies = df['companyid'].unique()\n",
    "    print(f\"Found {len(companies)} companies to process\")\n",
    "    \n",
    "    processed_dfs = []\n",
    "    skipped_companies = []\n",
    "    error_companies = []\n",
    "    \n",
    "    for i, company_id in enumerate(companies, 1):\n",
    "        # print(f\"Processing company {i}/{len(companies)}: {company_id}\")\n",
    "        \n",
    "        # Filter data for this company\n",
    "        company_data = df[df['companyid'] == company_id].copy()\n",
    "        \n",
    "        # Check data quality before processing\n",
    "        if len(company_data) < 50:\n",
    "            print(f\"  Skipping company {company_id} - insufficient data ({len(company_data)} rows)\")\n",
    "            skipped_companies.append({'company_id': company_id, 'reason': 'insufficient_data', 'rows': len(company_data)})\n",
    "            continue\n",
    "        \n",
    "        # Check for completely missing OHLC data\n",
    "        required_cols = ['high', 'low', 'close', 'open']\n",
    "        missing_all = all(company_data[col].isna().all() for col in required_cols)\n",
    "        if missing_all:\n",
    "            print(f\"  Skipping company {company_id} - all OHLC data is missing\")\n",
    "            skipped_companies.append({'company_id': company_id, 'reason': 'all_ohlc_missing', 'rows': len(company_data)})\n",
    "            continue\n",
    "        \n",
    "        # Sort by date if date column exists\n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "        company_data = company_data.reset_index(drop=True)\n",
    "        \n",
    "        # Calculate technical indicators for this company\n",
    "        try:\n",
    "            processed_company = calculate_indicators_for_company(company_data)\n",
    "            \n",
    "            if processed_company is not None:\n",
    "                processed_dfs.append(processed_company)\n",
    "                # print(f\"  Successfully processed company {company_id}\")\n",
    "            else:\n",
    "                print(f\"  Failed to process company {company_id} - returned None\")\n",
    "                error_companies.append({'company_id': company_id, 'reason': 'processing_failed'})\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing company {company_id}: {str(e)}\")\n",
    "            error_companies.append({'company_id': company_id, 'reason': f'exception: {str(e)}'})\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del company_data\n",
    "        if i % 10 == 0:  # Garbage collect every 10 companies\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  Successfully processed: {len(processed_dfs)} companies\")\n",
    "    print(f\"  Skipped: {len(skipped_companies)} companies\")\n",
    "    print(f\"  Errors: {len(error_companies)} companies\")\n",
    "    \n",
    "    if skipped_companies:\n",
    "        print(f\"\\nSkipped companies details:\")\n",
    "        for skip in skipped_companies[:5]:  # Show first 5\n",
    "            print(f\"  - {skip['company_id']}: {skip['reason']} ({skip.get('rows', 'N/A')} rows)\")\n",
    "        if len(skipped_companies) > 5:\n",
    "            print(f\"  ... and {len(skipped_companies) - 5} more\")\n",
    "    \n",
    "    if error_companies:\n",
    "        print(f\"\\nError companies details:\")\n",
    "        for error in error_companies[:5]:  # Show first 5\n",
    "            print(f\"  - {error['company_id']}: {error['reason']}\")\n",
    "        if len(error_companies) > 5:\n",
    "            print(f\"  ... and {len(error_companies) - 5} more\")\n",
    "    \n",
    "    if processed_dfs:\n",
    "        print(\"Combining all company data...\")\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No companies were successfully processed!\")\n",
    "        return None\n",
    "\n",
    "def calculate_indicators_for_company(company_data):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for a single company's data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Data validation and cleaning\n",
    "        # print(f\"  Data validation for company...\")\n",
    "        \n",
    "        # Check for missing OHLC data\n",
    "        required_cols = ['high', 'low', 'close', 'open']\n",
    "        missing_data_summary = {}\n",
    "        \n",
    "        for col in required_cols:\n",
    "            missing_count = company_data[col].isna().sum()\n",
    "            missing_pct = (missing_count / len(company_data)) * 100\n",
    "            missing_data_summary[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "            \n",
    "            if missing_pct > 50:\n",
    "                print(f\"  WARNING: {col} has {missing_pct:.1f}% missing values\")\n",
    "        \n",
    "        # Forward fill missing values for OHLC data (common practice)\n",
    "        for col in required_cols:\n",
    "            if company_data[col].isna().any():\n",
    "                print(f\"  Forward filling missing {col} values...\")\n",
    "                company_data[col] = company_data[col].fillna(method='ffill')\n",
    "                # If still NaN at the beginning, backward fill\n",
    "                company_data[col] = company_data[col].fillna(method='bfill')\n",
    "        \n",
    "        # Handle volume data separately\n",
    "        if 'volume' in company_data.columns:\n",
    "            volume_missing = company_data['volume'].isna().sum()\n",
    "            if volume_missing > 0:\n",
    "                print(f\"  Volume has {volume_missing} missing values, filling with 0...\")\n",
    "                company_data['volume'] = company_data['volume'].fillna(0)\n",
    "        \n",
    "        # Validate data consistency (High >= Low, etc.)\n",
    "        invalid_data = (company_data['high'] < company_data['low']).sum()\n",
    "        if invalid_data > 0:\n",
    "            print(f\"  WARNING: Found {invalid_data} rows where High < Low, fixing...\")\n",
    "            # Swap high and low where high < low\n",
    "            mask = company_data['high'] < company_data['low']\n",
    "            company_data.loc[mask, ['high', 'low']] = company_data.loc[mask, ['low', 'high']].values\n",
    "        \n",
    "        # Check for negative prices\n",
    "        negative_prices = (company_data[required_cols] < 0).any(axis=1).sum()\n",
    "        if negative_prices > 0:\n",
    "            print(f\"  WARNING: Found {negative_prices} rows with negative prices, removing...\")\n",
    "            company_data = company_data[(company_data[required_cols] >= 0).all(axis=1)]\n",
    "        \n",
    "        # Final check for sufficient data\n",
    "        if len(company_data) < 30:\n",
    "            print(f\"  Insufficient data after cleaning ({len(company_data)} rows)\")\n",
    "            return None\n",
    "        \n",
    "        high = company_data['high']\n",
    "        low = company_data['low']\n",
    "        close = company_data['close']\n",
    "        open_price = company_data['open']\n",
    "        \n",
    "        # Create result dataframe\n",
    "        result_df = company_data.copy()\n",
    "        \n",
    "        # Moving Averages\n",
    "        result_df['SMA_10'] = calculate_sma(close, 10)\n",
    "        result_df['SMA_20'] = calculate_sma(close, 20)\n",
    "        result_df['SMA_50'] = calculate_sma(close, 50)\n",
    "        result_df['EMA_12'] = calculate_ema(close, 12)\n",
    "        result_df['EMA_26'] = calculate_ema(close, 26)\n",
    "        result_df['EMA_50'] = calculate_ema(close, 50)\n",
    "        \n",
    "        # MACD\n",
    "        macd_line, signal_line, histogram = calculate_macd(close)\n",
    "        result_df['MACD'] = macd_line\n",
    "        result_df['MACD_Signal'] = signal_line\n",
    "        result_df['MACD_Histogram'] = histogram\n",
    "        \n",
    "        # RSI\n",
    "        result_df['RSI'] = calculate_rsi(close)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(close)\n",
    "        result_df['BB_Upper'] = bb_upper\n",
    "        result_df['BB_Middle'] = bb_middle\n",
    "        result_df['BB_Lower'] = bb_lower\n",
    "        result_df['BB_Width'] = bb_upper - bb_lower\n",
    "        result_df['BB_Position'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # Stochastic Oscillator\n",
    "        stoch_k, stoch_d = calculate_stochastic(high, low, close)\n",
    "        result_df['Stoch_K'] = stoch_k\n",
    "        result_df['Stoch_D'] = stoch_d\n",
    "        \n",
    "        # ATR\n",
    "        result_df['ATR'] = calculate_atr(high, low, close)\n",
    "        \n",
    "        # ADX\n",
    "        adx, di_plus, di_minus = calculate_adx(high, low, close)\n",
    "        result_df['ADX'] = adx\n",
    "        result_df['DI_Plus'] = di_plus\n",
    "        result_df['DI_Minus'] = di_minus\n",
    "        \n",
    "        # Parabolic SAR\n",
    "        if len(close) > 50:  # Only calculate if sufficient data\n",
    "            result_df['PSAR'] = calculate_parabolic_sar(high, low)\n",
    "        else:\n",
    "            result_df['PSAR'] = np.nan\n",
    "        \n",
    "        # Ichimoku Cloud\n",
    "        tenkan, kijun, senkou_a, senkou_b, chikou = calculate_ichimoku(high, low, close)\n",
    "        result_df['Ichimoku_Tenkan'] = tenkan\n",
    "        result_df['Ichimoku_Kijun'] = kijun\n",
    "        result_df['Ichimoku_Senkou_A'] = senkou_a\n",
    "        result_df['Ichimoku_Senkou_B'] = senkou_b\n",
    "        result_df['Ichimoku_Chikou'] = chikou\n",
    "        \n",
    "        # Rate of Change\n",
    "        result_df['ROC'] = calculate_roc(close)\n",
    "        \n",
    "        # CCI\n",
    "        result_df['CCI'] = calculate_cci(high, low, close)\n",
    "        \n",
    "        # Volume indicators (if volume data is available)\n",
    "        if 'volume' in company_data.columns and not company_data['volume'].isna().all():\n",
    "            volume = company_data['volume']\n",
    "            result_df['OBV'] = calculate_obv(close, volume)\n",
    "            result_df['AD_Line'] = calculate_ad_line(high, low, close, volume)\n",
    "            \n",
    "            # Volume Moving Averages\n",
    "            result_df['Volume_SMA_20'] = calculate_sma(volume, 20)\n",
    "            result_df['Volume_Ratio'] = volume / result_df['Volume_SMA_20'].replace(0, np.nan)\n",
    "            \n",
    "            # Volume Profile (only if sufficient data)\n",
    "            if len(close) > 50 and not volume.isna().all():\n",
    "                vol_at_price, dominant_price = calculate_volume_profile(close, volume)\n",
    "                result_df['Volume_At_Price'] = vol_at_price\n",
    "                result_df['Dominant_Price_Level'] = dominant_price\n",
    "            else:\n",
    "                result_df['Volume_At_Price'] = np.nan\n",
    "                result_df['Dominant_Price_Level'] = np.nan\n",
    "        else:\n",
    "            # Set volume indicators to NaN if no volume data\n",
    "            result_df['OBV'] = np.nan\n",
    "            result_df['AD_Line'] = np.nan\n",
    "            result_df['Volume_SMA_20'] = np.nan\n",
    "            result_df['Volume_Ratio'] = np.nan\n",
    "            result_df['Volume_At_Price'] = np.nan\n",
    "            result_df['Dominant_Price_Level'] = np.nan\n",
    "        \n",
    "        # Pivot Points\n",
    "        pivot, r1, r2, r3, s1, s2, s3 = calculate_pivot_points(\n",
    "            high.shift(1), low.shift(1), close.shift(1)\n",
    "        )\n",
    "        result_df['Pivot'] = pivot\n",
    "        result_df['Resistance_1'] = r1\n",
    "        result_df['Resistance_2'] = r2\n",
    "        result_df['Resistance_3'] = r3\n",
    "        result_df['Support_1'] = s1\n",
    "        result_df['Support_2'] = s2\n",
    "        result_df['Support_3'] = s3\n",
    "        \n",
    "        # Fibonacci Retracement Levels\n",
    "        fib_236, fib_382, fib_500, fib_618, fib_786 = add_fibonacci_levels(high, low)\n",
    "        result_df['Fib_23.6'] = fib_236\n",
    "        result_df['Fib_38.2'] = fib_382\n",
    "        result_df['Fib_50.0'] = fib_500\n",
    "        result_df['Fib_61.8'] = fib_618\n",
    "        result_df['Fib_78.6'] = fib_786\n",
    "        \n",
    "        # Additional Price-based indicators\n",
    "        result_df['Price_Change'] = close.pct_change()\n",
    "        result_df['Price_Change_abs'] = abs(result_df['Price_Change'])\n",
    "        result_df['High_Low_Ratio'] = high / low\n",
    "        result_df['Close_Open_Ratio'] = close / open_price\n",
    "        \n",
    "        # True Range\n",
    "        result_df['True_Range'] = calculate_atr(high, low, close, window=1)\n",
    "        \n",
    "        # Technical Indicators - Additional\n",
    "        result_df['TSI'] = calculate_tsi(close)\n",
    "        result_df['MFI'] = calculate_mfi(high, low, close, volume)\n",
    "        result_df['PVT'] = calculate_pvt(close, volume)\n",
    "        result_df['TEMA'] = calculate_tema(close)\n",
    "        result_df['KAMA'] = calculate_kama(close)\n",
    "        result_df['Ulcer_Index'] = calculate_ulcer_index(close)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing company data: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_tsi(close, first_smooth=25, second_smooth=13):\n",
    "    \"\"\"True Strength Index (TSI)\"\"\"\n",
    "    if len(close) < max(first_smooth, second_smooth) + 10:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate price momentum\n",
    "    momentum = close.diff()\n",
    "    \n",
    "    # Double smoothing of momentum\n",
    "    momentum_smoothed_1 = momentum.ewm(span=first_smooth).mean()\n",
    "    momentum_smoothed_2 = momentum_smoothed_1.ewm(span=second_smooth).mean()\n",
    "    \n",
    "    # Double smoothing of absolute momentum\n",
    "    abs_momentum = momentum.abs()\n",
    "    abs_momentum_smoothed_1 = abs_momentum.ewm(span=first_smooth).mean()\n",
    "    abs_momentum_smoothed_2 = abs_momentum_smoothed_1.ewm(span=second_smooth).mean()\n",
    "    \n",
    "    # Calculate TSI\n",
    "    tsi = 100 * (momentum_smoothed_2 / abs_momentum_smoothed_2.replace(0, np.nan))\n",
    "    return tsi\n",
    "\n",
    "def calculate_mfi(high, low, close, volume, window=14):\n",
    "    \"\"\"Money Flow Index (MFI)\"\"\"\n",
    "    if volume.isna().all() or len(close) < window + 5:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean data\n",
    "    clean_high = high.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_low = low.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_close = close.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_volume = volume.fillna(0)\n",
    "    \n",
    "    # Calculate typical price\n",
    "    typical_price = (clean_high + clean_low + clean_close) / 3\n",
    "    \n",
    "    # Calculate raw money flow\n",
    "    raw_money_flow = typical_price * clean_volume\n",
    "    \n",
    "    # Calculate positive and negative money flow\n",
    "    price_change = typical_price.diff()\n",
    "    positive_flow = pd.Series(np.where(price_change > 0, raw_money_flow, 0), index=close.index)\n",
    "    negative_flow = pd.Series(np.where(price_change < 0, raw_money_flow, 0), index=close.index)\n",
    "    \n",
    "    # Calculate MFI\n",
    "    positive_mf = positive_flow.rolling(window=window).sum()\n",
    "    negative_mf = negative_flow.rolling(window=window).sum()\n",
    "    \n",
    "    money_ratio = positive_mf / negative_mf.replace(0, np.nan)\n",
    "    mfi = 100 - (100 / (1 + money_ratio))\n",
    "    \n",
    "    return mfi\n",
    "\n",
    "def calculate_pvt(close, volume):\n",
    "    \"\"\"Price Volume Trend (PVT)\"\"\"\n",
    "    if volume.isna().all() or close.isna().all():\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Clean data\n",
    "    clean_close = close.fillna(method='ffill').fillna(method='bfill')\n",
    "    clean_volume = volume.fillna(0)\n",
    "    \n",
    "    # Calculate price change percentage\n",
    "    price_change_pct = clean_close.pct_change()\n",
    "    \n",
    "    # Calculate PVT\n",
    "    pvt_change = price_change_pct * clean_volume\n",
    "    pvt = pvt_change.cumsum()\n",
    "    \n",
    "    return pvt\n",
    "\n",
    "def calculate_tema(data, window=14):\n",
    "    \"\"\"Triple Exponential Moving Average (TEMA)\"\"\"\n",
    "    if len(data) < window * 3:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    # First EMA\n",
    "    ema1 = data.ewm(span=window).mean()\n",
    "    \n",
    "    # Second EMA (EMA of EMA1)\n",
    "    ema2 = ema1.ewm(span=window).mean()\n",
    "    \n",
    "    # Third EMA (EMA of EMA2)\n",
    "    ema3 = ema2.ewm(span=window).mean()\n",
    "    \n",
    "    # TEMA formula\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    \n",
    "    return tema\n",
    "\n",
    "def calculate_kama(data, window=14, fast_sc=2, slow_sc=30):\n",
    "    \"\"\"Kaufman's Adaptive Moving Average (KAMA)\"\"\"\n",
    "    if len(data) < window + 10:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    # Calculate change and volatility\n",
    "    change = abs(data - data.shift(window))\n",
    "    volatility = data.diff().abs().rolling(window=window).sum()\n",
    "    \n",
    "    # Calculate efficiency ratio\n",
    "    efficiency_ratio = change / volatility.replace(0, np.nan)\n",
    "    \n",
    "    # Calculate smoothing constant\n",
    "    fast_sc_eff = 2.0 / (fast_sc + 1)\n",
    "    slow_sc_eff = 2.0 / (slow_sc + 1)\n",
    "    sc = (efficiency_ratio * (fast_sc_eff - slow_sc_eff) + slow_sc_eff) ** 2\n",
    "    \n",
    "    # Calculate KAMA\n",
    "    kama = pd.Series(index=data.index, dtype=float)\n",
    "    kama.iloc[window-1] = data.iloc[window-1]  # Initial value\n",
    "    \n",
    "    for i in range(window, len(data)):\n",
    "        if not pd.isna(sc.iloc[i]):\n",
    "            kama.iloc[i] = kama.iloc[i-1] + sc.iloc[i] * (data.iloc[i] - kama.iloc[i-1])\n",
    "        else:\n",
    "            kama.iloc[i] = kama.iloc[i-1]\n",
    "    \n",
    "    return kama\n",
    "\n",
    "def calculate_ulcer_index(close, window=14):\n",
    "    \"\"\"Ulcer Index - Volatility measure focusing on downside risk\"\"\"\n",
    "    if len(close) < window + 5:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate percentage drawdowns\n",
    "    rolling_max = close.rolling(window=window).max()\n",
    "    drawdowns = ((close - rolling_max) / rolling_max) * 100\n",
    "    \n",
    "    # Calculate squared drawdowns\n",
    "    squared_drawdowns = drawdowns ** 2\n",
    "    \n",
    "    # Calculate Ulcer Index\n",
    "    ulcer_index = np.sqrt(squared_drawdowns.rolling(window=window).mean())\n",
    "    \n",
    "    return ulcer_index\n",
    "\n",
    "def generate_data_quality_report(df):\n",
    "    \"\"\"Generate a comprehensive data quality report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Total companies: {df['companyid'].nunique() if 'companyid' in df.columns else 'N/A'}\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\" if hasattr(df.index, 'min') else \"Date info not available\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    print(f\"\\nMISSING DATA ANALYSIS:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_pct = (missing_summary / len(df)) * 100\n",
    "    \n",
    "    # Show columns with missing data\n",
    "    cols_with_missing = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "    if len(cols_with_missing) > 0:\n",
    "        print(f\"Columns with missing data:\")\n",
    "        for col, count in cols_with_missing.head(10).items():\n",
    "            pct = missing_pct[col]\n",
    "            print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "        if len(cols_with_missing) > 10:\n",
    "            print(f\"  ... and {len(cols_with_missing) - 10} more columns\")\n",
    "    else:\n",
    "        print(\"  No missing data found!\")\n",
    "    \n",
    "    # Technical indicators completeness\n",
    "    print(f\"\\nTECHNICAL INDICATORS COMPLETENESS:\")\n",
    "    indicator_cols = [col for col in df.columns if col not in ['companyid', 'companyName', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    \n",
    "    if indicator_cols:\n",
    "        print(f\"Total indicators calculated: {len(indicator_cols)}\")\n",
    "        \n",
    "        # Check which indicators have the most complete data\n",
    "        indicator_completeness = {}\n",
    "        for col in indicator_cols:\n",
    "            completeness = ((df[col].notna().sum() / len(df)) * 100)\n",
    "            indicator_completeness[col] = completeness\n",
    "        \n",
    "        # Sort by completeness\n",
    "        sorted_indicators = sorted(indicator_completeness.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Top 10 most complete indicators:\")\n",
    "        for indicator, completeness in sorted_indicators[:10]:\n",
    "            print(f\"  {indicator}: {completeness:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nBottom 5 least complete indicators:\")\n",
    "        for indicator, completeness in sorted_indicators[-5:]:\n",
    "            print(f\"  {indicator}: {completeness:.1f}%\")\n",
    "    \n",
    "    # Data range validation\n",
    "    print(f\"\\nDATA VALIDATION SUMMARY:\")\n",
    "    if 'close' in df.columns:\n",
    "        print(f\"Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "        negative_prices = (df['close'] < 0).sum()\n",
    "        if negative_prices > 0:\n",
    "            print(f\"WARNING: {negative_prices} negative close prices found\")\n",
    "    \n",
    "    if 'volume' in df.columns:\n",
    "        print(f\"Volume range: {df['volume'].min():,.0f} - {df['volume'].max():,.0f}\")\n",
    "        zero_volume = (df['volume'] == 0).sum()\n",
    "        print(f\"Zero volume records: {zero_volume:,} ({(zero_volume/len(df)*100):.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths (relative to project root)\n",
    "    input_file = \"../data/raw/priceData5Year.csv\"\n",
    "    output_file = \"../data/processed/stock_data_with_technical_indicators.csv\"\n",
    "    \n",
    "    # For large datasets (>1M rows), use chunk processing\n",
    "    # Set chunk_size=None to load entire dataset at once\n",
    "    chunk_size = 500000  # Process 500k rows at a time\n",
    "    \n",
    "    print(f\"Processing {input_file}...\")\n",
    "    print(\"Note: This is a large dataset. Processing may take several minutes...\")\n",
    "    \n",
    "    # Create the technical indicators dataset\n",
    "    enhanced_df = create_technical_indicators_dataset(input_file, chunk_size=chunk_size)\n",
    "    \n",
    "    if enhanced_df is not None:\n",
    "        # Save the enhanced dataset\n",
    "        print(f\"Saving enhanced dataset to {output_file}...\")\n",
    "        enhanced_df.to_csv(output_file)\n",
    "        \n",
    "        # Generate data quality report first\n",
    "        generate_data_quality_report(enhanced_df)\n",
    "        \n",
    "        # Save the enhanced dataset\n",
    "        print(f\"\\nSaving enhanced dataset to {output_file}...\")\n",
    "        enhanced_df.to_csv(output_file)\n",
    "        \n",
    "        print(\"\\nDataset Summary:\")\n",
    "        print(f\"Total rows: {enhanced_df.shape[0]:,}\")\n",
    "        print(f\"Total columns: {enhanced_df.shape[1]}\")\n",
    "        \n",
    "        print(\"\\nTechnical Indicators Added:\")\n",
    "        original_cols = ['companyid', 'companyName', 'open', 'high', 'low', 'close', 'volume']\n",
    "        new_cols = [col for col in enhanced_df.columns if col not in original_cols]\n",
    "        for i, col in enumerate(new_cols, 1):\n",
    "            print(f\"{i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\nEnhanced dataset saved successfully as '{output_file}'\")\n",
    "        \n",
    "        # Show sample of the data (for first company if multi-company dataset)\n",
    "        if 'companyid' in enhanced_df.columns:\n",
    "            first_company = enhanced_df['companyid'].iloc[0]\n",
    "            sample_data = enhanced_df[enhanced_df['companyid'] == first_company].head()\n",
    "            print(f\"\\nSample data for company {first_company} (first 5 rows):\")\n",
    "            key_cols = ['open', 'high', 'low', 'close', 'RSI', 'MACD', 'BB_Position']\n",
    "            available_cols = [col for col in key_cols if col in sample_data.columns]\n",
    "            print(sample_data[available_cols].round(4).to_string())\n",
    "        else:\n",
    "            print(\"\\nSample of the enhanced dataset (first 5 rows):\")\n",
    "            key_cols = ['open', 'high', 'low', 'close', 'RSI', 'MACD', 'BB_Position']\n",
    "            available_cols = [col for col in key_cols if col in enhanced_df.columns]\n",
    "            print(enhanced_df[available_cols].head().round(4).to_string())\n",
    "        \n",
    "        # Show basic statistics\n",
    "        print(\"\\nBasic statistics for key indicators:\")\n",
    "        key_indicators = ['RSI', 'MACD', 'BB_Position', 'ADX', 'ATR']\n",
    "        available_indicators = [col for col in key_indicators if col in enhanced_df.columns]\n",
    "        if available_indicators:\n",
    "            stats_df = enhanced_df[available_indicators].describe()\n",
    "            print(stats_df.round(4).to_string())\n",
    "    else:\n",
    "        print(\"Failed to create the technical indicators dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TREND INDICATORS =====\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Moving Average Convergence Divergence\"\"\"\n",
    "    ema_fast = calculate_ema(data, fast)\n",
    "    ema_slow = calculate_ema(data, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = calculate_ema(macd_line, signal)\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20, std_dev=2):\n",
    "    \"\"\"Bollinger Bands\"\"\"\n",
    "    sma = calculate_sma(data, window)\n",
    "    std = data.rolling(window=window).std()\n",
    "    upper_band = sma + (std * std_dev)\n",
    "    lower_band = sma - (std * std_dev)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def calculate_tema(data, window=14):\n",
    "    \"\"\"Triple Exponential Moving Average (TEMA)\"\"\"\n",
    "    if len(data) < window * 3:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    # First EMA\n",
    "    ema1 = data.ewm(span=window).mean()\n",
    "    \n",
    "    # Second EMA (EMA of EMA1)\n",
    "    ema2 = ema1.ewm(span=window).mean()\n",
    "    \n",
    "    # Third EMA (EMA of EMA2)\n",
    "    ema3 = ema2.ewm(span=window).mean()\n",
    "    \n",
    "    # TEMA formula\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    \n",
    "    return tema\n",
    "\n",
    "def calculate_kama(data, window=14, fast_sc=2, slow_sc=30):\n",
    "    \"\"\"Kaufman's Adaptive Moving Average (KAMA)\"\"\"\n",
    "    if len(data) < window + 10:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    # Calculate change and volatility\n",
    "    change = abs(data - data.shift(window))\n",
    "    volatility = data.diff().abs().rolling(window=window).sum()\n",
    "    \n",
    "    # Calculate efficiency ratio\n",
    "    efficiency_ratio = change / volatility.replace(0, np.nan)\n",
    "    \n",
    "    # Calculate smoothing constant\n",
    "    fast_sc_eff = 2.0 / (fast_sc + 1)\n",
    "    slow_sc_eff = 2.0 / (slow_sc + 1)\n",
    "    sc = (efficiency_ratio * (fast_sc_eff - slow_sc_eff) + slow_sc_eff) ** 2\n",
    "    \n",
    "    # Calculate KAMA\n",
    "    kama = pd.Series(index=data.index, dtype=float)\n",
    "    kama.iloc[window-1] = data.iloc[window-1]  # Initial value\n",
    "    \n",
    "    for i in range(window, len(data)):\n",
    "        if not pd.isna(sc.iloc[i]):\n",
    "            kama.iloc[i] = kama.iloc[i-1] + sc.iloc[i] * (data.iloc[i] - kama.iloc[i-1])\n",
    "        else:\n",
    "            kama.iloc[i] = kama.iloc[i-1]\n",
    "    \n",
    "    return kama\n",
    "\n",
    "# ===== BASIC TECHNICAL INDICATORS =====\n",
    "\n",
    "def calculate_sma(data, window):\n",
    "    \"\"\"Simple Moving Average\"\"\"\n",
    "    return data.rolling(window=window).mean()\n",
    "\n",
    "def calculate_ema(data, window):\n",
    "    \"\"\"Exponential Moving Average\"\"\"\n",
    "    return data.ewm(span=window).mean()\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    \"\"\"Relative Strength Index with Wilder's smoothing\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    alpha = 1.0 / window\n",
    "    avg_gain = gain.ewm(alpha=alpha, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_roc(data, window=12):\n",
    "    \"\"\"Rate of Change\"\"\"\n",
    "    return ((data - data.shift(window)) / data.shift(window)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OSCILLATOR INDICATORS =====\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_window=14, d_window=3):\n",
    "    \"\"\"Stochastic Oscillator\"\"\"\n",
    "    lowest_low = low.rolling(window=k_window).min()\n",
    "    highest_high = high.rolling(window=k_window).max()\n",
    "    k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))\n",
    "    d_percent = k_percent.rolling(window=d_window).mean()\n",
    "    return k_percent, d_percent\n",
    "\n",
    "def calculate_tsi(data, long_window=25, short_window=13):\n",
    "    \"\"\"True Strength Index\"\"\"\n",
    "    if len(data) < long_window + short_window:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    # Calculate price change\n",
    "    price_change = data.diff()\n",
    "    \n",
    "    # Double smoothed price change\n",
    "    first_smooth = price_change.ewm(span=long_window).mean()\n",
    "    double_smooth = first_smooth.ewm(span=short_window).mean()\n",
    "    \n",
    "    # Double smoothed absolute price change\n",
    "    abs_price_change = abs(price_change)\n",
    "    abs_first_smooth = abs_price_change.ewm(span=long_window).mean()\n",
    "    abs_double_smooth = abs_first_smooth.ewm(span=short_window).mean()\n",
    "    \n",
    "    # Calculate TSI\n",
    "    tsi = 100 * (double_smooth / abs_double_smooth.replace(0, np.nan))\n",
    "    \n",
    "    return tsi\n",
    "\n",
    "# ===== TREND INDICATORS =====\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Moving Average Convergence Divergence\"\"\"\n",
    "    ema_fast = calculate_ema(data, fast)\n",
    "    ema_slow = calculate_ema(data, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = calculate_ema(macd_line, signal)\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20, std_dev=2):\n",
    "    \"\"\"Bollinger Bands\"\"\"\n",
    "    sma = calculate_sma(data, window)\n",
    "    std = data.rolling(window=window).std()\n",
    "    upper_band = sma + (std * std_dev)\n",
    "    lower_band = sma - (std * std_dev)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def calculate_tema(data, window=14):\n",
    "    \"\"\"Triple Exponential Moving Average (TEMA)\"\"\"\n",
    "    if len(data) < window * 3:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    ema1 = data.ewm(span=window).mean()\n",
    "    ema2 = ema1.ewm(span=window).mean()\n",
    "    ema3 = ema2.ewm(span=window).mean()\n",
    "    tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "    return tema\n",
    "\n",
    "def calculate_kama(data, window=14, fast_sc=2, slow_sc=30):\n",
    "    \"\"\"Kaufman's Adaptive Moving Average (KAMA)\"\"\"\n",
    "    if len(data) < window + 10:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    change = abs(data - data.shift(window))\n",
    "    volatility = data.diff().abs().rolling(window=window).sum()\n",
    "    efficiency_ratio = change / volatility.replace(0, np.nan)\n",
    "    \n",
    "    fast_sc_eff = 2.0 / (fast_sc + 1)\n",
    "    slow_sc_eff = 2.0 / (slow_sc + 1)\n",
    "    sc = (efficiency_ratio * (fast_sc_eff - slow_sc_eff) + slow_sc_eff) ** 2\n",
    "    \n",
    "    kama = pd.Series(index=data.index, dtype=float)\n",
    "    kama.iloc[window-1] = data.iloc[window-1]\n",
    "    \n",
    "    for i in range(window, len(data)):\n",
    "        if not pd.isna(sc.iloc[i]):\n",
    "            kama.iloc[i] = kama.iloc[i-1] + sc.iloc[i] * (data.iloc[i] - kama.iloc[i-1])\n",
    "        else:\n",
    "            kama.iloc[i] = kama.iloc[i-1]\n",
    "    \n",
    "    return kama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VOLATILITY INDICATORS =====\n",
    "\n",
    "def calculate_atr(high, low, close, window=14):\n",
    "    \"\"\"Average True Range\"\"\"\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    return atr\n",
    "\n",
    "def calculate_adx(high, low, close, window=14):\n",
    "    \"\"\"Average Directional Index with Wilder's smoothing\"\"\"\n",
    "    # Calculate True Range\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Calculate Directional Movement using pandas operations to maintain index alignment\n",
    "    high_diff = high - high.shift()\n",
    "    low_diff = low.shift() - low\n",
    "    \n",
    "    dm_plus = pd.Series(np.where(high_diff > low_diff, np.maximum(high_diff, 0), 0), index=high.index)\n",
    "    dm_minus = pd.Series(np.where(low_diff > high_diff, np.maximum(low_diff, 0), 0), index=high.index)\n",
    "    \n",
    "    # Use Wilder's smoothing (exponential smoothing with alpha = 1/window)\n",
    "    alpha = 1.0 / window\n",
    "    tr_smooth = tr.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_plus_smooth = dm_plus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_minus_smooth = dm_minus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    # Calculate DI+ and DI- with safe division\n",
    "    di_plus = 100 * (dm_plus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    di_minus = 100 * (dm_minus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    \n",
    "    # Calculate DX and ADX with safe division\n",
    "    di_sum = di_plus + di_minus\n",
    "    dx = 100 * abs(di_plus - di_minus) / di_sum.replace(0, np.nan)\n",
    "    adx = dx.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    return adx, di_plus, di_minus\n",
    "\n",
    "def calculate_ulcer_index(close, window=14):\n",
    "    \"\"\"Ulcer Index - Volatility measure focusing on downside risk\"\"\"\n",
    "    if len(close) < window + 5:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate percentage drawdowns\n",
    "    rolling_max = close.rolling(window=window).max()\n",
    "    drawdowns = ((close - rolling_max) / rolling_max) * 100\n",
    "    \n",
    "    # Calculate squared drawdowns\n",
    "    squared_drawdowns = drawdowns ** 2\n",
    "    \n",
    "    # Calculate Ulcer Index\n",
    "    ulcer_index = np.sqrt(squared_drawdowns.rolling(window=window).mean())\n",
    "    \n",
    "    return ulcer_index\n",
    "\n",
    "# ===== OSCILLATOR INDICATORS =====\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_window=14, d_window=3):\n",
    "    \"\"\"Stochastic Oscillator\"\"\"\n",
    "    lowest_low = low.rolling(window=k_window).min()\n",
    "    highest_high = high.rolling(window=k_window).max()\n",
    "    k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))\n",
    "    d_percent = k_percent.rolling(window=d_window).mean()\n",
    "    return k_percent, d_percent\n",
    "\n",
    "def calculate_tsi(data, long_window=25, short_window=13):\n",
    "    \"\"\"True Strength Index\"\"\"\n",
    "    if len(data) < long_window + short_window:\n",
    "        return pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    price_change = data.diff()\n",
    "    first_smooth = price_change.ewm(span=long_window).mean()\n",
    "    double_smooth = first_smooth.ewm(span=short_window).mean()\n",
    "    \n",
    "    abs_price_change = abs(price_change)\n",
    "    abs_first_smooth = abs_price_change.ewm(span=long_window).mean()\n",
    "    abs_double_smooth = abs_first_smooth.ewm(span=short_window).mean()\n",
    "    \n",
    "    tsi = 100 * (double_smooth / abs_double_smooth.replace(0, np.nan))\n",
    "    return tsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VOLUME INDICATORS =====\n",
    "\n",
    "def calculate_obv(close, volume):\n",
    "    \"\"\"On Balance Volume\"\"\"\n",
    "    if len(close) < 2:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate price direction\n",
    "    direction = np.where(close > close.shift(), 1, np.where(close < close.shift(), -1, 0))\n",
    "    \n",
    "    # Calculate OBV\n",
    "    obv = (direction * volume).cumsum()\n",
    "    \n",
    "    return pd.Series(obv, index=close.index)\n",
    "\n",
    "def calculate_mfi(high, low, close, volume, window=14):\n",
    "    \"\"\"Money Flow Index\"\"\"\n",
    "    if len(close) < window + 1:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate typical price\n",
    "    typical_price = (high + low + close) / 3\n",
    "    \n",
    "    # Calculate money flow\n",
    "    money_flow = typical_price * volume\n",
    "    \n",
    "    # Calculate positive and negative money flow\n",
    "    positive_flow = money_flow.where(typical_price > typical_price.shift(), 0)\n",
    "    negative_flow = money_flow.where(typical_price < typical_price.shift(), 0)\n",
    "    \n",
    "    # Calculate money flow ratio\n",
    "    positive_mf = positive_flow.rolling(window=window).sum()\n",
    "    negative_mf = negative_flow.rolling(window=window).sum()\n",
    "    \n",
    "    # Calculate MFI\n",
    "    money_ratio = positive_mf / negative_mf.replace(0, np.nan)\n",
    "    mfi = 100 - (100 / (1 + money_ratio))\n",
    "    \n",
    "    return mfi\n",
    "\n",
    "def calculate_pvt(close, volume):\n",
    "    \"\"\"Price Volume Trend\"\"\"\n",
    "    if len(close) < 2:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    # Calculate price change percentage\n",
    "    price_change_pct = close.pct_change()\n",
    "    \n",
    "    # Calculate PVT\n",
    "    pvt = (price_change_pct * volume).cumsum()\n",
    "    \n",
    "    return pvt\n",
    "\n",
    "# ===== VOLATILITY INDICATORS =====\n",
    "\n",
    "def calculate_atr(high, low, close, window=14):\n",
    "    \"\"\"Average True Range\"\"\"\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    return atr\n",
    "\n",
    "def calculate_adx(high, low, close, window=14):\n",
    "    \"\"\"Average Directional Index with Wilder's smoothing\"\"\"\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    high_diff = high - high.shift()\n",
    "    low_diff = low.shift() - low\n",
    "    \n",
    "    dm_plus = pd.Series(np.where(high_diff > low_diff, np.maximum(high_diff, 0), 0), index=high.index)\n",
    "    dm_minus = pd.Series(np.where(low_diff > high_diff, np.maximum(low_diff, 0), 0), index=high.index)\n",
    "    \n",
    "    alpha = 1.0 / window\n",
    "    tr_smooth = tr.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_plus_smooth = dm_plus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    dm_minus_smooth = dm_minus.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    di_plus = 100 * (dm_plus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    di_minus = 100 * (dm_minus_smooth / tr_smooth.replace(0, np.nan))\n",
    "    \n",
    "    di_sum = di_plus + di_minus\n",
    "    dx = 100 * abs(di_plus - di_minus) / di_sum.replace(0, np.nan)\n",
    "    adx = dx.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    return adx, di_plus, di_minus\n",
    "\n",
    "def calculate_ulcer_index(close, window=14):\n",
    "    \"\"\"Ulcer Index - Volatility measure focusing on downside risk\"\"\"\n",
    "    if len(close) < window + 5:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    rolling_max = close.rolling(window=window).max()\n",
    "    drawdowns = ((close - rolling_max) / rolling_max) * 100\n",
    "    squared_drawdowns = drawdowns ** 2\n",
    "    ulcer_index = np.sqrt(squared_drawdowns.rolling(window=window).mean())\n",
    "    return ulcer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58031a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BASIC TECHNICAL INDICATORS =====\n",
    "\n",
    "def calculate_sma(data, window):\n",
    "    \"\"\"Simple Moving Average\"\"\"\n",
    "    return data.rolling(window=window).mean()\n",
    "\n",
    "def calculate_ema(data, window):\n",
    "    \"\"\"Exponential Moving Average\"\"\"\n",
    "    return data.ewm(span=window).mean()\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Moving Average Convergence Divergence\"\"\"\n",
    "    ema_fast = calculate_ema(data, fast)\n",
    "    ema_slow = calculate_ema(data, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = calculate_ema(macd_line, signal)\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    \"\"\"Relative Strength Index with Wilder's smoothing\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    # Use Wilder's smoothing (exponential smoothing with alpha = 1/window)\n",
    "    alpha = 1.0 / window\n",
    "    avg_gain = gain.ewm(alpha=alpha, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=alpha, adjust=False).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20, std_dev=2):\n",
    "    \"\"\"Bollinger Bands\"\"\"\n",
    "    sma = calculate_sma(data, window)\n",
    "    std = data.rolling(window=window).std()\n",
    "    upper_band = sma + (std * std_dev)\n",
    "    lower_band = sma - (std * std_dev)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_window=14, d_window=3):\n",
    "    \"\"\"Stochastic Oscillator\"\"\"\n",
    "    lowest_low = low.rolling(window=k_window).min()\n",
    "    highest_high = high.rolling(window=k_window).max()\n",
    "    k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))\n",
    "    d_percent = k_percent.rolling(window=d_window).mean()\n",
    "    return k_percent, d_percent\n",
    "\n",
    "def calculate_atr(high, low, close, window=14):\n",
    "    \"\"\"Average True Range\"\"\"\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    return atr\n",
    "\n",
    "print(\"âœ… Basic technical indicators functions defined\")\n",
    "\n",
    "# ===== DATA LOADING AND VALIDATION =====\n",
    "\n",
    "def load_and_validate_data(file_path, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Load data from CSV file and perform basic validation\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data with optional chunking for large files\n",
    "        if chunk_size:\n",
    "            print(f\"Loading large dataset in chunks of {chunk_size:,} rows...\")\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "            first_chunk = next(chunk_iter)\n",
    "            print(f\"First chunk loaded. Shape: {first_chunk.shape}\")\n",
    "            \n",
    "            # Load remaining chunks\n",
    "            all_chunks = [first_chunk]\n",
    "            for i, chunk in enumerate(chunk_iter, 2):\n",
    "                print(f\"Loading chunk {i}...\")\n",
    "                all_chunks.append(chunk)\n",
    "            \n",
    "            df = pd.concat(all_chunks, ignore_index=True)\n",
    "            print(f\"All chunks concatenated. Final shape: {df.shape}\")\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_and_map_columns(df):\n",
    "    \"\"\"\n",
    "    Auto-detect column names and create mapping\n",
    "    \"\"\"\n",
    "    print(\"Detecting column mappings...\")\n",
    "    \n",
    "    # Auto-detect column names (case insensitive)\n",
    "    columns_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'date' in col_lower or 'time' in col_lower:\n",
    "            columns_map['date'] = col\n",
    "        elif 'open' in col_lower:\n",
    "            columns_map['open'] = col\n",
    "        elif 'high' in col_lower:\n",
    "            columns_map['high'] = col\n",
    "        elif 'low' in col_lower:\n",
    "            columns_map['low'] = col\n",
    "        elif 'close' in col_lower:\n",
    "            columns_map['close'] = col\n",
    "        elif 'volume' in col_lower or 'quantity' in col_lower or 'traded' in col_lower:\n",
    "            columns_map['volume'] = col\n",
    "    \n",
    "    print(f\"Detected columns mapping: {columns_map}\")\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    required_cols = ['open', 'high', 'low', 'close']\n",
    "    missing_cols = [col for col in required_cols if col not in columns_map]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing required columns: {missing_cols}\")\n",
    "        return None, None\n",
    "    \n",
    "    return columns_map, required_cols\n",
    "\n",
    "print(\"âœ… Data loading and validation functions defined\")\n",
    "\n",
    "# ===== VOLUME INDICATORS =====\n",
    "\n",
    "def calculate_obv(close, volume):\n",
    "    \"\"\"On Balance Volume\"\"\"\n",
    "    if len(close) < 2:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    direction = np.where(close > close.shift(), 1, np.where(close < close.shift(), -1, 0))\n",
    "    obv = (direction * volume).cumsum()\n",
    "    return pd.Series(obv, index=close.index)\n",
    "\n",
    "def calculate_mfi(high, low, close, volume, window=14):\n",
    "    \"\"\"Money Flow Index\"\"\"\n",
    "    if len(close) < window + 1:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    typical_price = (high + low + close) / 3\n",
    "    money_flow = typical_price * volume\n",
    "    \n",
    "    positive_flow = money_flow.where(typical_price > typical_price.shift(), 0)\n",
    "    negative_flow = money_flow.where(typical_price < typical_price.shift(), 0)\n",
    "    \n",
    "    positive_mf = positive_flow.rolling(window=window).sum()\n",
    "    negative_mf = negative_flow.rolling(window=window).sum()\n",
    "    \n",
    "    money_ratio = positive_mf / negative_mf.replace(0, np.nan)\n",
    "    mfi = 100 - (100 / (1 + money_ratio))\n",
    "    return mfi\n",
    "\n",
    "def calculate_pvt(close, volume):\n",
    "    \"\"\"Price Volume Trend\"\"\"\n",
    "    if len(close) < 2:\n",
    "        return pd.Series(index=close.index, dtype=float)\n",
    "    \n",
    "    price_change_pct = close.pct_change()\n",
    "    pvt = (price_change_pct * volume).cumsum()\n",
    "    return pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA LOADING AND VALIDATION =====\n",
    "\n",
    "def load_and_validate_data(file_path, chunk_size=None):\n",
    "    \"\"\"Load data from CSV file and perform basic validation\"\"\"\n",
    "    try:\n",
    "        if chunk_size:\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "            first_chunk = next(chunk_iter)\n",
    "            all_chunks = [first_chunk]\n",
    "            for chunk in chunk_iter:\n",
    "                all_chunks.append(chunk)\n",
    "            df = pd.concat(all_chunks, ignore_index=True)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "def detect_and_map_columns(df):\n",
    "    \"\"\"Auto-detect column names and create mapping\"\"\"\n",
    "    columns_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'date' in col_lower or 'time' in col_lower:\n",
    "            columns_map['date'] = col\n",
    "        elif 'open' in col_lower:\n",
    "            columns_map['open'] = col\n",
    "        elif 'high' in col_lower:\n",
    "            columns_map['high'] = col\n",
    "        elif 'low' in col_lower:\n",
    "            columns_map['low'] = col\n",
    "        elif 'close' in col_lower:\n",
    "            columns_map['close'] = col\n",
    "        elif 'volume' in col_lower or 'quantity' in col_lower or 'traded' in col_lower:\n",
    "            columns_map['volume'] = col\n",
    "    \n",
    "    required_cols = ['open', 'high', 'low', 'close']\n",
    "    missing_cols = [col for col in required_cols if col not in columns_map]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing required columns: {missing_cols}\")\n",
    "        return None, None\n",
    "    \n",
    "    return columns_map, required_cols\n",
    "\n",
    "# ===== DATA PREPROCESSING =====\n",
    "\n",
    "def preprocess_data(df, columns_map):\n",
    "    \"\"\"\n",
    "    Preprocess the data by renaming columns and converting data types\n",
    "    \"\"\"\n",
    "    # Rename columns for easier access\n",
    "    df_renamed = df.rename(columns={v: k for k, v in columns_map.items()})\n",
    "    \n",
    "    # Convert date column if it exists\n",
    "    if 'date' in columns_map:\n",
    "        df_renamed['date'] = pd.to_datetime(df_renamed['date'])\n",
    "        df_renamed = df_renamed.sort_values('date')\n",
    "        df_renamed = df_renamed.set_index('date')\n",
    "    \n",
    "    # Ensure numeric columns\n",
    "    numeric_cols = ['open', 'high', 'low', 'close']\n",
    "    if 'volume' in columns_map:\n",
    "        numeric_cols.append('volume')\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df_renamed[col] = pd.to_numeric(df_renamed[col], errors='coerce')\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "def validate_company_data(company_data, company_id=None):\n",
    "    \"\"\"\n",
    "    Validate data quality for a single company\n",
    "    \"\"\"\n",
    "    # Check data quality before processing\n",
    "    if len(company_data) < MIN_ROWS_PER_COMPANY:\n",
    "        if company_id:\n",
    "            print(f\"  Skipping company {company_id} - insufficient data ({len(company_data)} rows)\")\n",
    "        return False\n",
    "    \n",
    "    # Check for completely missing OHLC data\n",
    "    required_cols = ['high', 'low', 'close', 'open']\n",
    "    missing_all = all(company_data[col].isna().all() for col in required_cols)\n",
    "    if missing_all:\n",
    "        if company_id:\n",
    "            print(f\"  Skipping company {company_id} - all OHLC data is missing\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ… Data loading and preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b017bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TECHNICAL INDICATORS CALCULATION =====\n",
    "\n",
    "def calculate_indicators_for_company(company_data):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for a single company's data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Data validation and cleaning\n",
    "        required_cols = ['high', 'low', 'close', 'open']\n",
    "        \n",
    "        # Forward fill missing values for OHLC data (common practice)\n",
    "        for col in required_cols:\n",
    "            if company_data[col].isna().any():\n",
    "                company_data[col] = company_data[col].fillna(method='ffill')\n",
    "                # If still NaN at the beginning, backward fill\n",
    "                company_data[col] = company_data[col].fillna(method='bfill')\n",
    "        \n",
    "        # Handle volume data separately\n",
    "        if 'volume' in company_data.columns:\n",
    "            company_data['volume'] = company_data['volume'].fillna(0)\n",
    "        \n",
    "        # Final check for sufficient data\n",
    "        if len(company_data) < 30:\n",
    "            return None\n",
    "        \n",
    "        high = company_data['high']\n",
    "        low = company_data['low']\n",
    "        close = company_data['close']\n",
    "        open_price = company_data['open']\n",
    "        \n",
    "        # Create result dataframe\n",
    "        result_df = company_data.copy()\n",
    "        \n",
    "        # Calculate all technical indicators\n",
    "        \n",
    "        # Basic indicators\n",
    "        result_df['RSI'] = calculate_rsi(close)\n",
    "        result_df['ROC'] = calculate_roc(close)\n",
    "        \n",
    "        # Oscillators\n",
    "        stoch_k, stoch_d = calculate_stochastic(high, low, close)\n",
    "        result_df['Stoch_K'] = stoch_k\n",
    "        result_df['Stoch_D'] = stoch_d\n",
    "        result_df['TSI'] = calculate_tsi(close)\n",
    "        \n",
    "        # Trend indicators\n",
    "        macd_line, signal_line, histogram = calculate_macd(close)\n",
    "        result_df['MACD'] = macd_line\n",
    "        result_df['MACD_Signal'] = signal_line\n",
    "        result_df['MACD_Histogram'] = histogram\n",
    "        result_df['TEMA'] = calculate_tema(close)\n",
    "        result_df['KAMA'] = calculate_kama(close)\n",
    "        \n",
    "        # Volatility indicators\n",
    "        result_df['ATR'] = calculate_atr(high, low, close)\n",
    "        result_df['ADX'], _, _ = calculate_adx(high, low, close)\n",
    "        result_df['Ulcer_Index'] = calculate_ulcer_index(close)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(close)\n",
    "        result_df['BB_Upper'] = bb_upper\n",
    "        result_df['BB_Middle'] = bb_middle\n",
    "        result_df['BB_Lower'] = bb_lower\n",
    "        result_df['BB_Width'] = bb_upper - bb_lower\n",
    "        result_df['BB_Position'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # Volume indicators (if volume data is available)\n",
    "        if 'volume' in company_data.columns and not company_data['volume'].isna().all():\n",
    "            volume = company_data['volume']\n",
    "            result_df['OBV'] = calculate_obv(close, volume)\n",
    "            result_df['MFI'] = calculate_mfi(high, low, close, volume)\n",
    "            result_df['PVT'] = calculate_pvt(close, volume)\n",
    "            \n",
    "            # Volume Moving Averages\n",
    "            result_df['Volume_SMA_20'] = calculate_sma(volume, 20)\n",
    "            result_df['Volume_Ratio'] = volume / result_df['Volume_SMA_20'].replace(0, np.nan)\n",
    "        else:\n",
    "            # Set volume indicators to NaN if no volume data\n",
    "            result_df['OBV'] = np.nan\n",
    "            result_df['MFI'] = np.nan\n",
    "            result_df['PVT'] = np.nan\n",
    "            result_df['Volume_Ratio'] = np.nan\n",
    "        \n",
    "        # Additional indicators\n",
    "        result_df['Price_Change'] = close.pct_change()\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# ===== DATA PREPROCESSING =====\n",
    "\n",
    "def preprocess_data(df, columns_map):\n",
    "    \"\"\"Preprocess the data by renaming columns and converting data types\"\"\"\n",
    "    df_renamed = df.rename(columns={v: k for k, v in columns_map.items()})\n",
    "    \n",
    "    if 'date' in columns_map:\n",
    "        df_renamed['date'] = pd.to_datetime(df_renamed['date'])\n",
    "        df_renamed = df_renamed.sort_values('date')\n",
    "        df_renamed = df_renamed.set_index('date')\n",
    "    \n",
    "    numeric_cols = ['open', 'high', 'low', 'close']\n",
    "    if 'volume' in columns_map:\n",
    "        numeric_cols.append('volume')\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df_renamed[col] = pd.to_numeric(df_renamed[col], errors='coerce')\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "def validate_company_data(company_data, company_id=None):\n",
    "    \"\"\"Validate data quality for a single company\"\"\"\n",
    "    if len(company_data) < MIN_ROWS_PER_COMPANY:\n",
    "        return False\n",
    "    \n",
    "    required_cols = ['high', 'low', 'close', 'open']\n",
    "    missing_all = all(company_data[col].isna().all() for col in required_cols)\n",
    "    if missing_all:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a36526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MULTI-COMPANY PROCESSING =====\n",
    "\n",
    "def process_by_company(df):\n",
    "    \"\"\"Process technical indicators for each company separately\"\"\"\n",
    "    companies = df['companyid'].unique()\n",
    "    processed_dfs = []\n",
    "    \n",
    "    for i, company_id in enumerate(companies, 1):\n",
    "        company_data = df[df['companyid'] == company_id].copy()\n",
    "        \n",
    "        if not validate_company_data(company_data, company_id):\n",
    "            continue\n",
    "        \n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "        company_data = company_data.reset_index(drop=True)\n",
    "        \n",
    "        try:\n",
    "            processed_company = calculate_indicators_for_company(company_data)\n",
    "            if processed_company is not None:\n",
    "                processed_dfs.append(processed_company)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        del company_data\n",
    "        if i % 10 == 0:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN PROCESSING FUNCTION =====\n",
    "\n",
    "def create_technical_indicators_dataset(file_path, chunk_size=None):\n",
    "    \"\"\"Main function to create a comprehensive technical indicators dataset\"\"\"\n",
    "    df = load_and_validate_data(file_path, chunk_size)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    columns_map, required_cols = detect_and_map_columns(df)\n",
    "    if columns_map is None:\n",
    "        return None\n",
    "    \n",
    "    df_processed = preprocess_data(df, columns_map)\n",
    "    \n",
    "    if 'companyid' in df_processed.columns and len(df_processed['companyid'].unique()) > 1:\n",
    "        result_df = process_by_company(df_processed)\n",
    "    else:\n",
    "        result_df = calculate_indicators_for_company(df_processed)\n",
    "    \n",
    "    if result_df is None:\n",
    "        return None\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257baea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA QUALITY REPORTING =====\n",
    "\n",
    "def generate_data_quality_report(df):\n",
    "    \"\"\"Generate a comprehensive data quality report\"\"\"\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Total companies: {df['companyid'].nunique() if 'companyid' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_pct = (missing_summary / len(df)) * 100\n",
    "    \n",
    "    cols_with_missing = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "    if len(cols_with_missing) > 0:\n",
    "        print(\"Columns with missing data:\")\n",
    "        for col, count in cols_with_missing.head(10).items():\n",
    "            pct = missing_pct[col]\n",
    "            print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    indicator_cols = [col for col in df.columns if col not in ['companyid', 'companyName', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    print(f\"Technical indicators calculated: {len(indicator_cols)}\")\n",
    "    \n",
    "    if 'close' in df.columns:\n",
    "        print(f\"Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "    \n",
    "    if 'volume' in df.columns:\n",
    "        print(f\"Volume range: {df['volume'].min():,.0f} - {df['volume'].max():,.0f}\")\n",
    "\n",
    "print(\"âœ… Data quality reporting function defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
